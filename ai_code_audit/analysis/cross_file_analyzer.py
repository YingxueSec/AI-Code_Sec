#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·¨æ–‡ä»¶å…³è”åˆ†æå™¨
å½“å‘ç°ä¸ç¡®å®šæ¼æ´æ—¶ï¼Œè‡ªåŠ¨åˆ†æç›¸å…³æ–‡ä»¶è¿›è¡Œè¾…åŠ©åˆ¤å®š
"""

import re
import ast
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class RelatedFile:
    """ç›¸å…³æ–‡ä»¶ä¿¡æ¯"""
    path: str
    relationship: str  # 'caller', 'callee', 'config', 'template', 'parent'
    confidence: float
    reason: str

@dataclass
class CrossFileAnalysisResult:
    """è·¨æ–‡ä»¶åˆ†æç»“æœ"""
    original_confidence: float
    adjusted_confidence: float
    related_files: List[RelatedFile]
    evidence: List[str]
    recommendation: str

class CrossFileAnalyzer:
    """è·¨æ–‡ä»¶å…³è”åˆ†æå™¨"""
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.file_cache = {}  # æ–‡ä»¶å†…å®¹ç¼“å­˜
        self.analysis_cache = {}  # æ–°å¢ï¼šåˆ†æç»“æœç¼“å­˜
        self.search_cache = {}    # æ–°å¢ï¼šæœç´¢ç»“æœç¼“å­˜
        self.semaphore = asyncio.Semaphore(2)  # é™åˆ¶è·¨æ–‡ä»¶åˆ†æå¹¶å‘æ•°
        
    async def analyze_uncertain_finding(
        self,
        finding: Dict[str, Any],
        file_path: str,
        code: str,
        llm_manager,
        analysis_stack: Optional[Set[str]] = None,
        max_depth: int = 3
    ) -> CrossFileAnalysisResult:
        """
        åˆ†æä¸ç¡®å®šçš„æ¼æ´å‘ç°ï¼Œé€šè¿‡å…³è”æ–‡ä»¶è¿›è¡Œè¾…åŠ©åˆ¤å®š

        Args:
            finding: æ¼æ´å‘ç°ä¿¡æ¯
            file_path: å½“å‰æ–‡ä»¶è·¯å¾„
            code: å½“å‰æ–‡ä»¶ä»£ç 
            llm_manager: LLMç®¡ç†å™¨ï¼Œç”¨äºåˆ†æå…³è”æ–‡ä»¶

        Returns:
            CrossFileAnalysisResult: è·¨æ–‡ä»¶åˆ†æç»“æœ
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(finding, file_path)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.analysis_cache:
            logger.info(f"Using cached cross-file analysis result for {file_path}")
            return self.analysis_cache[cache_key]

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        async with self.semaphore:
            # æ‰§è¡Œåˆ†æ
            result = await self._perform_analysis(finding, file_path, code, llm_manager, analysis_stack, max_depth)

            # ç¼“å­˜ç»“æœ
            self.analysis_cache[cache_key] = result

            return result

    async def _perform_analysis(
        self,
        finding: Dict[str, Any],
        file_path: str,
        code: str,
        llm_manager,
        analysis_stack: Optional[Set[str]] = None,
        max_depth: int = 3
    ) -> CrossFileAnalysisResult:
        """æ‰§è¡Œå®é™…çš„è·¨æ–‡ä»¶åˆ†æ"""
        from ..utils.recursion_monitor import RecursionGuard, AnalysisType

        # ä½¿ç”¨é€’å½’ä¿æŠ¤
        async with RecursionGuard(file_path, AnalysisType.CROSS_FILE):
            # åˆå§‹åŒ–åˆ†æå †æ ˆï¼Œç”¨äºé˜²æ­¢æ— é™é€’å½’
            if analysis_stack is None:
                analysis_stack = set()

            # å°†å½“å‰æ–‡ä»¶åŠ å…¥å †æ ˆ
            analysis_stack.add(file_path)

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é€’å½’æ·±åº¦
            if len(analysis_stack) > max_depth:
                return CrossFileAnalysisResult(
                    original_confidence=finding.get('confidence', 0.5),
                    adjusted_confidence=finding.get('confidence', 0.5),
                    related_files=[],
                    evidence=["Cross-file analysis stopped: maximum recursion depth reached."],
                    recommendation="é€’å½’åˆ†ææ·±åº¦è¶…é™ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–ï¼Œè¯·æ‰‹åŠ¨å®¡æŸ¥ã€‚"
                )

            original_confidence = finding.get('confidence', 0.5)

            # å¯¹éœ€è¦è·¨æ–‡ä»¶åˆ†æçš„é—®é¢˜è¿›è¡Œå¤„ç†
            # è°ƒæ•´é˜ˆå€¼ï¼Œå…è®¸æ›´å¤šé—®é¢˜è¿›è¡Œè·¨æ–‡ä»¶åˆ†æ
            if original_confidence > 0.99:
                return CrossFileAnalysisResult(
                    original_confidence=original_confidence,
                    adjusted_confidence=original_confidence,
                    related_files=[],
                    evidence=[],
                    recommendation="æé«˜ç½®ä¿¡åº¦é—®é¢˜ï¼Œæ— éœ€è·¨æ–‡ä»¶åˆ†æ"
                )

            # 1. è¯†åˆ«ç›¸å…³æ–‡ä»¶
            related_files = await self._find_related_files(finding, file_path, code)

            # 2. åˆ†æç›¸å…³æ–‡ä»¶
            evidence = []
            confidence_adjustments = []

            for related_file in related_files:
                try:
                    # å¦‚æœå…³è”æ–‡ä»¶å·²åœ¨åˆ†æå †æ ˆä¸­ï¼Œåˆ™è·³è¿‡ï¼Œé¿å…å¾ªç¯
                    if related_file.path in analysis_stack:
                        continue

                    analysis = await self._analyze_related_file(
                        related_file, finding, llm_manager, analysis_stack, max_depth
                    )
                    evidence.extend(analysis['evidence'])
                    confidence_adjustments.append(analysis['confidence_adjustment'])

                except Exception as e:
                    print(f"Warning: Failed to analyze related file {related_file.path}: {e}")

            # 3. è®¡ç®—è°ƒæ•´åçš„ç½®ä¿¡åº¦
            adjusted_confidence = self._calculate_adjusted_confidence(
                original_confidence, confidence_adjustments
            )

            # 4. ç”Ÿæˆå»ºè®®
            recommendation = self._generate_recommendation(
                finding, evidence, original_confidence, adjusted_confidence
            )

            return CrossFileAnalysisResult(
                original_confidence=original_confidence,
                adjusted_confidence=adjusted_confidence,
                related_files=related_files,
                evidence=evidence,
                recommendation=recommendation
            )
    
    async def _find_related_files(
        self, 
        finding: Dict[str, Any], 
        file_path: str, 
        code: str
    ) -> List[RelatedFile]:
        """æŸ¥æ‰¾ç›¸å…³æ–‡ä»¶"""
        related_files = []
        finding_type = finding.get('type', '')
        
        # 1. æŸ¥æ‰¾è°ƒç”¨è€…æ–‡ä»¶
        callers = self._find_caller_files(file_path, code)
        related_files.extend(callers)
        
        # 2. æŸ¥æ‰¾è¢«è°ƒç”¨æ–‡ä»¶
        callees = self._find_callee_files(file_path, code)
        related_files.extend(callees)
        
        # 3. æŸ¥æ‰¾é…ç½®æ–‡ä»¶
        if 'è·¯å¾„éå†' in finding_type or 'æ–‡ä»¶ä¸Šä¼ ' in finding_type:
            config_files = self._find_config_files(file_path)
            related_files.extend(config_files)
        
        # 4. æŸ¥æ‰¾æ¨¡æ¿æ–‡ä»¶
        if 'XSS' in finding_type:
            template_files = self._find_template_files(file_path, code)
            related_files.extend(template_files)
        
        # 5. æŸ¥æ‰¾çˆ¶çº§æ§åˆ¶å™¨
        if 'upload' in file_path.lower() or 'file' in file_path.lower():
            parent_files = self._find_parent_controller_files(file_path)
            related_files.extend(parent_files)
        
        return related_files[:5]  # é™åˆ¶æœ€å¤š5ä¸ªç›¸å…³æ–‡ä»¶
    
    def _find_caller_files(self, file_path: str, code: str) -> List[RelatedFile]:
        """æŸ¥æ‰¾è°ƒç”¨å½“å‰æ–‡ä»¶çš„æ–‡ä»¶"""
        related_files = []
        current_file = Path(file_path)
        
        # æœç´¢å¯èƒ½çš„è°ƒç”¨è€…
        search_patterns = [
            current_file.stem,  # æ–‡ä»¶å
            current_file.name,  # å®Œæ•´æ–‡ä»¶å
        ]
        
        # åœ¨é¡¹ç›®ä¸­æœç´¢å¼•ç”¨
        for pattern in search_patterns:
            for found_file in self._search_files_containing(pattern):
                if found_file != file_path:
                    related_files.append(RelatedFile(
                        path=found_file,
                        relationship='caller',
                        confidence=0.7,
                        reason=f"æ–‡ä»¶ä¸­åŒ…å«å¯¹{pattern}çš„å¼•ç”¨"
                    ))
        
        return related_files[:3]
    
    def _find_callee_files(self, file_path: str, code: str) -> List[RelatedFile]:
        """æŸ¥æ‰¾å½“å‰æ–‡ä»¶è°ƒç”¨çš„æ–‡ä»¶"""
        related_files = []
        
        # åˆ†æä»£ç ä¸­çš„åŒ…å«/å¼•ç”¨
        include_patterns = [
            r'include\s*[\'"]([^\'"]+)[\'"]',
            r'require\s*[\'"]([^\'"]+)[\'"]',
            r'import\s+[\'"]([^\'"]+)[\'"]',
            r'from\s+[\'"]([^\'"]+)[\'"]',
        ]
        
        for pattern in include_patterns:
            matches = re.findall(pattern, code, re.IGNORECASE)
            for match in matches:
                # æ„å»ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„
                possible_paths = self._resolve_file_path(file_path, match)
                for path in possible_paths:
                    if path.exists():
                        related_files.append(RelatedFile(
                            path=str(path),
                            relationship='callee',
                            confidence=0.8,
                            reason=f"é€šè¿‡{pattern}å¼•ç”¨"
                        ))
        
        return related_files[:3]
    
    def _find_config_files(self, file_path: str) -> List[RelatedFile]:
        """æŸ¥æ‰¾é…ç½®æ–‡ä»¶"""
        related_files = []
        
        # å¸¸è§é…ç½®æ–‡ä»¶
        config_patterns = [
            '**/config.php',
            '**/config.ini',
            '**/settings.py',
            '**/application.properties',
            '**/web.xml',
            '**/.htaccess'
        ]
        
        for pattern in config_patterns:
            for config_file in self.project_path.rglob(pattern):
                related_files.append(RelatedFile(
                    path=str(config_file),
                    relationship='config',
                    confidence=0.6,
                    reason="é¡¹ç›®é…ç½®æ–‡ä»¶ï¼Œå¯èƒ½åŒ…å«å®‰å…¨è®¾ç½®"
                ))
        
        return related_files[:2]
    
    def _find_template_files(self, file_path: str, code: str) -> List[RelatedFile]:
        """æŸ¥æ‰¾æ¨¡æ¿æ–‡ä»¶"""
        related_files = []
        
        # æŸ¥æ‰¾æ¨¡æ¿å¼•ç”¨
        template_patterns = [
            r'template\s*[\'"]([^\'"]+)[\'"]',
            r'render\s*[\'"]([^\'"]+)[\'"]',
            r'include\s*[\'"]([^\'"]+\.html?)[\'"]',
        ]
        
        for pattern in template_patterns:
            matches = re.findall(pattern, code, re.IGNORECASE)
            for match in matches:
                possible_paths = self._resolve_file_path(file_path, match)
                for path in possible_paths:
                    if path.exists():
                        related_files.append(RelatedFile(
                            path=str(path),
                            relationship='template',
                            confidence=0.7,
                            reason=f"æ¨¡æ¿æ–‡ä»¶å¼•ç”¨: {match}"
                        ))
        
        return related_files[:2]
    
    def _find_parent_controller_files(self, file_path: str) -> List[RelatedFile]:
        """æŸ¥æ‰¾çˆ¶çº§æ§åˆ¶å™¨æ–‡ä»¶"""
        related_files = []
        current_path = Path(file_path)
        
        # å‘ä¸ŠæŸ¥æ‰¾æ§åˆ¶å™¨æ–‡ä»¶
        for parent in current_path.parents:
            for controller_file in parent.glob('*Controller*'):
                if controller_file.is_file():
                    related_files.append(RelatedFile(
                        path=str(controller_file),
                        relationship='parent',
                        confidence=0.6,
                        reason="çˆ¶çº§æ§åˆ¶å™¨ï¼Œå¯èƒ½åŒ…å«æƒé™éªŒè¯"
                    ))
        
        return related_files[:2]
    
    async def _analyze_related_file(
        self,
        related_file: RelatedFile,
        finding: Dict[str, Any],
        llm_manager,
        analysis_stack: Set[str],
        max_depth: int
    ) -> Dict[str, Any]:
        """åˆ†æç›¸å…³æ–‡ä»¶"""
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            if related_file.path not in self.file_cache:
                with open(related_file.path, 'r', encoding='utf-8', errors='ignore') as f:
                    self.file_cache[related_file.path] = f.read()
            
            related_code = self.file_cache[related_file.path]
            
            # æ„å»ºé’ˆå¯¹æ€§çš„åˆ†ææç¤º
            analysis_prompt = self._build_related_file_analysis_prompt(
                finding, related_file, related_code
            )
            
            # ä½¿ç”¨LLMåˆ†æç›¸å…³æ–‡ä»¶
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ analysis_context å‚æ•°é˜²æ­¢é€’å½’
            result = await llm_manager.analyze_code(
                code=related_code,
                file_path=related_file.path,
                language=self._detect_language(related_file.path),
                template="related_file_analysis",
                prompt_override=analysis_prompt,
                analysis_context="related_file"  # ğŸ”¥ æ–°å¢ï¼šæ ‡è®°ä¸ºå…³è”æ–‡ä»¶åˆ†æ
            )
            
            if result.get('success'):
                findings = result.get('findings', [])
                return self._extract_evidence_from_findings(findings, finding)
            else:
                return {'evidence': [], 'confidence_adjustment': 0}
                
        except Exception as e:
            print(f"Error analyzing related file {related_file.path}: {e}")
            return {'evidence': [], 'confidence_adjustment': 0}
    
    def _build_related_file_analysis_prompt(
        self, 
        original_finding: Dict[str, Any], 
        related_file: RelatedFile, 
        related_code: str
    ) -> str:
        """æ„å»ºç›¸å…³æ–‡ä»¶åˆ†ææç¤º"""
        finding_type = original_finding.get('type', '')
        
        if 'è·¯å¾„éå†' in finding_type:
            return f"""
è¯·åˆ†ææ­¤æ–‡ä»¶æ˜¯å¦åŒ…å«ä»¥ä¸‹å®‰å…¨æ§åˆ¶ï¼š
1. è·¯å¾„éªŒè¯å’Œè¿‡æ»¤æœºåˆ¶
2. ç™½åå•ç›®å½•é™åˆ¶
3. æ–‡ä»¶ä¸Šä¼ å®‰å…¨é…ç½®
4. æƒé™éªŒè¯æœºåˆ¶

é‡ç‚¹å…³æ³¨æ˜¯å¦æœ‰ä»£ç å¯ä»¥ç¼“è§£è·¯å¾„éå†é£é™©ã€‚
"""
        elif 'XSS' in finding_type:
            return f"""
è¯·åˆ†ææ­¤æ–‡ä»¶æ˜¯å¦åŒ…å«ä»¥ä¸‹å®‰å…¨æ§åˆ¶ï¼š
1. è¾“å‡ºè½¬ä¹‰å’Œè¿‡æ»¤
2. CSP (Content Security Policy) é…ç½®
3. XSSé˜²æŠ¤æœºåˆ¶
4. è¾“å…¥éªŒè¯

é‡ç‚¹å…³æ³¨æ˜¯å¦æœ‰ä»£ç å¯ä»¥é˜²æ­¢XSSæ”»å‡»ã€‚
"""
        else:
            return f"""
è¯·åˆ†ææ­¤æ–‡ä»¶æ˜¯å¦åŒ…å«ä¸{finding_type}ç›¸å…³çš„å®‰å…¨æ§åˆ¶æœºåˆ¶ã€‚
"""
    
    def _extract_evidence_from_findings(
        self, 
        findings: List[Dict], 
        original_finding: Dict
    ) -> Dict[str, Any]:
        """ä»ç›¸å…³æ–‡ä»¶çš„å‘ç°ä¸­æå–è¯æ®"""
        evidence = []
        confidence_adjustment = 0
        
        original_type = original_finding.get('type', '')
        
        for finding in findings:
            finding_type = finding.get('type', '')
            
            # å¦‚æœç›¸å…³æ–‡ä»¶ä¸­å‘ç°äº†ç›¸åŒç±»å‹çš„é—®é¢˜ï¼Œå¢åŠ ç½®ä¿¡åº¦
            if finding_type == original_type:
                confidence_adjustment += 0.2
                evidence.append(f"ç›¸å…³æ–‡ä»¶ä¸­å‘ç°ç›¸åŒç±»å‹é—®é¢˜: {finding.get('description', '')[:100]}")
            
            # å¦‚æœç›¸å…³æ–‡ä»¶ä¸­å‘ç°äº†å®‰å…¨æ§åˆ¶ï¼Œé™ä½ç½®ä¿¡åº¦
            elif 'å®‰å…¨' in finding.get('description', '') or 'éªŒè¯' in finding.get('description', ''):
                confidence_adjustment -= 0.1
                evidence.append(f"ç›¸å…³æ–‡ä»¶ä¸­å‘ç°å®‰å…¨æ§åˆ¶: {finding.get('description', '')[:100]}")
        
        return {
            'evidence': evidence,
            'confidence_adjustment': confidence_adjustment
        }
    
    def _calculate_adjusted_confidence(
        self, 
        original_confidence: float, 
        adjustments: List[float]
    ) -> float:
        """è®¡ç®—è°ƒæ•´åçš„ç½®ä¿¡åº¦"""
        total_adjustment = sum(adjustments)
        adjusted = original_confidence + total_adjustment
        return max(0.1, min(1.0, adjusted))
    
    def _generate_recommendation(
        self, 
        finding: Dict[str, Any], 
        evidence: List[str], 
        original_confidence: float, 
        adjusted_confidence: float
    ) -> str:
        """ç”Ÿæˆå»ºè®®"""
        if adjusted_confidence > original_confidence + 0.1:
            return f"è·¨æ–‡ä»¶åˆ†æå¢åŠ äº†é—®é¢˜çš„ç½®ä¿¡åº¦ ({original_confidence:.2f} â†’ {adjusted_confidence:.2f})ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤"
        elif adjusted_confidence < original_confidence - 0.1:
            return f"è·¨æ–‡ä»¶åˆ†æé™ä½äº†é—®é¢˜çš„ç½®ä¿¡åº¦ ({original_confidence:.2f} â†’ {adjusted_confidence:.2f})ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨æ§åˆ¶"
        else:
            return "è·¨æ–‡ä»¶åˆ†ææœªæ˜¾è‘—æ”¹å˜ç½®ä¿¡åº¦ï¼Œå»ºè®®è¿›ä¸€æ­¥äººå·¥å®¡æŸ¥"
    
    def _search_files_containing(self, pattern: str) -> List[str]:
        """ä¼˜åŒ–çš„æ–‡ä»¶æœç´¢ï¼Œé¿å…å…¨é¡¹ç›®æ‰«æ"""
        found_files = []

        # æ£€æŸ¥æœç´¢ç¼“å­˜
        if hasattr(self, 'search_cache') and pattern in self.search_cache:
            logger.info(f"Using cached search result for pattern '{pattern}'")
            return self.search_cache[pattern]

        # 1. é™åˆ¶æœç´¢èŒƒå›´å’Œæ–‡ä»¶ç±»å‹
        search_extensions = {'.php', '.java', '.py', '.js', '.html', '.jsp', '.xml'}
        max_search_files = 100  # é™åˆ¶æœç´¢æ–‡ä»¶æ•°é‡
        max_results = 5  # é™åˆ¶ç»“æœæ•°é‡

        searched_count = 0

        try:
            for file_path in self.project_path.rglob('*'):
                if searched_count >= max_search_files:
                    break

                if not file_path.is_file() or file_path.suffix not in search_extensions:
                    continue

                # è·³è¿‡å¤§æ–‡ä»¶
                try:
                    if file_path.stat().st_size > 500 * 1024:  # 500KBé™åˆ¶
                        continue
                except:
                    continue

                searched_count += 1

                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        # åªè¯»å–å‰10KBå†…å®¹è¿›è¡Œæœç´¢
                        content = f.read(10240)
                        if pattern in content:
                            found_files.append(str(file_path))
                            if len(found_files) >= max_results:
                                break
                except:
                    continue

        except Exception as e:
            logger.warning(f"File search failed: {e}")

        # ç¼“å­˜æœç´¢ç»“æœ
        if not hasattr(self, 'search_cache'):
            self.search_cache = {}
        self.search_cache[pattern] = found_files

        logger.info(f"Searched {searched_count} files, found {len(found_files)} matches for pattern '{pattern}'")
        return found_files

    def _generate_cache_key(self, finding: Dict[str, Any], file_path: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_data = f"{file_path}:{finding.get('type', '')}:{finding.get('line', 0)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _resolve_file_path(self, current_file: str, relative_path: str) -> List[Path]:
        """è§£æç›¸å¯¹æ–‡ä»¶è·¯å¾„"""
        current_path = Path(current_file)
        possible_paths = []
        
        # ç›¸å¯¹äºå½“å‰æ–‡ä»¶
        possible_paths.append(current_path.parent / relative_path)
        
        # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        possible_paths.append(self.project_path / relative_path)
        
        # æ·»åŠ å¸¸è§æ‰©å±•å
        for path in possible_paths.copy():
            if not path.suffix:
                for ext in ['.php', '.html', '.js', '.py']:
                    possible_paths.append(path.with_suffix(ext))
        
        return [p for p in possible_paths if p.exists()]
    
    def _detect_language(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶è¯­è¨€"""
        extension = Path(file_path).suffix.lower()
        language_map = {
            '.php': 'php',
            '.java': 'java',
            '.py': 'python',
            '.js': 'javascript',
            '.html': 'html',
            '.xml': 'xml'
        }
        return language_map.get(extension, 'unknown')
