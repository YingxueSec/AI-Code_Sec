#!/usr/bin/env python3
"""
qdbcrm-v3.0.2 å®Œæ•´ä»£ç å®¡è®¡è„šæœ¬
åŸºäºæˆ‘ä»¬éªŒè¯è¿‡çš„ä¿®å¤é€»è¾‘ï¼Œæä¾›å®Œæ•´çš„å®‰å…¨å®¡è®¡åŠŸèƒ½
"""
import os
import sys
import time
import json
import asyncio
import aiohttp
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""
    path: str
    full_path: str
    size: int
    size_mb: float
    language: str = "php"

@dataclass
class SecurityIssue:
    """å®‰å…¨é—®é¢˜"""
    type: str
    severity: str
    description: str
    line_number: Optional[int] = None
    code_snippet: Optional[str] = None

@dataclass
class AuditResult:
    """å®¡è®¡ç»“æœ"""
    file_path: str
    language: str
    issues: List[SecurityIssue]
    summary: str
    lines_analyzed: int
    analysis_time: float

class SmartFileFilter:
    """æ™ºèƒ½æ–‡ä»¶è¿‡æ»¤å™¨ - åŸºäºæˆ‘ä»¬éªŒè¯çš„ä¿®å¤é€»è¾‘"""

    def __init__(self):
        # åŸºäºæµ‹è¯•éªŒè¯çš„è¿‡æ»¤è§„åˆ™
        self.ignore_patterns = [
            '*.js', '*.css', '*.min.js', '*.min.css',
            '*.jpg', '*.png', '*.gif', '*.ico', '*.svg',
            '*.woff', '*.ttf', '*.eot',
            '*.json', '*.xml', '*.txt', '*.md', '*.log',
            '*.zip', '*.tar', '*.gz'
        ]

        self.ignore_dirs = [
            'install', 'theme', 'vendor', 'node_modules',
            '.git', '.idea', '.vscode', 'cache', 'logs'
        ]

        self.max_file_size = 10 * 1024 * 1024  # 10MB

    def should_filter_file(self, file_path: str, project_root: str) -> tuple[bool, str]:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
        rel_path = os.path.relpath(file_path, project_root)

        # æ‰©å±•åè¿‡æ»¤
        for pattern in self.ignore_patterns:
            if file_path.endswith(pattern.replace('*', '')):
                return True, f'æ‰©å±•åè¿‡æ»¤: {pattern}'

        # ç›®å½•è¿‡æ»¤
        for ignore_dir in self.ignore_dirs:
            if ignore_dir in rel_path:
                return True, f'ç›®å½•è¿‡æ»¤: {ignore_dir}'

        # æ–‡ä»¶å¤§å°è¿‡æ»¤
        try:
            if os.path.getsize(file_path) > self.max_file_size:
                return True, f'æ–‡ä»¶è¿‡å¤§: >{self.max_file_size/1024/1024:.1f}MB'
        except:
            pass

        # åªä¿ç•™PHPæ–‡ä»¶
        if not file_path.endswith('.php'):
            return True, 'éPHPæ–‡ä»¶'

        return False, 'éœ€è¦å®¡è®¡'

class LargeFileHandler:
    """å¤§æ–‡ä»¶å¤„ç†å™¨ - åŸºäºæˆ‘ä»¬éªŒè¯çš„åˆ†å—é€»è¾‘"""

    def __init__(self, chunk_size: int = 50000):
        self.chunk_size = chunk_size

    def should_chunk_file(self, file_size: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å—"""
        return file_size > 3 * 1024 * 1024  # 3MB

    def chunk_file_content(self, content: str) -> List[str]:
        """å°†æ–‡ä»¶å†…å®¹åˆ†å—"""
        if len(content) <= self.chunk_size:
            return [content]

        chunks = []
        lines = content.split('\n')
        current_chunk = []
        current_size = 0

        for line in lines:
            line_size = len(line) + 1  # +1 for newline

            if current_size + line_size > self.chunk_size and current_chunk:
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size

        if current_chunk:
            chunks.append('\n'.join(current_chunk))

        return chunks

class SecurityAnalyzer:
    """å®‰å…¨åˆ†æå™¨ - åŸºäºæˆ‘ä»¬éªŒè¯çš„æ£€æµ‹é€»è¾‘"""

    def __init__(self):
        self.security_patterns = {
            'SQLæ³¨å…¥é£é™©': {
                'patterns': ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'mysql_query', 'mysqli_query'],
                'severity': 'High'
            },
            'XSSé£é™©': {
                'patterns': ['echo', 'print', '$_GET', '$_POST', '$_REQUEST'],
                'severity': 'High'
            },
            'æ–‡ä»¶æ“ä½œé£é™©': {
                'patterns': ['include', 'require', 'file_get_contents', 'fopen'],
                'severity': 'Medium'
            },
            'ä»£ç æ‰§è¡Œé£é™©': {
                'patterns': ['eval', 'exec', 'system', 'shell_exec'],
                'severity': 'Critical'
            },
            'è®¤è¯ç»•è¿‡é£é™©': {
                'patterns': ['password', 'login', 'auth', 'session'],
                'severity': 'High'
            },
            'æ–‡ä»¶ä¸Šä¼ é£é™©': {
                'patterns': ['upload', 'move_uploaded_file', '$_FILES'],
                'severity': 'High'
            },
            'ä¿¡æ¯æ³„éœ²é£é™©': {
                'patterns': ['phpinfo', 'var_dump', 'print_r'],
                'severity': 'Medium'
            },
            'æƒé™æ§åˆ¶é£é™©': {
                'patterns': ['admin', 'role', 'permission', 'access'],
                'severity': 'Medium'
            }
        }

    def analyze_content(self, content: str, file_path: str) -> List[SecurityIssue]:
        """åˆ†ææ–‡ä»¶å†…å®¹"""
        issues = []
        lines = content.split('\n')

        for category, config in self.security_patterns.items():
            patterns = config['patterns']
            severity = config['severity']

            for i, line in enumerate(lines, 1):
                line_lower = line.lower()
                for pattern in patterns:
                    if pattern.lower() in line_lower:
                        issue = SecurityIssue(
                            type=category,
                            severity=severity,
                            description=f'å‘ç° {pattern} æ¨¡å¼',
                            line_number=i,
                            code_snippet=line.strip()[:100]
                        )
                        issues.append(issue)

        return issues

class QdbcrmAuditor:
    """qdbcrmå®Œæ•´å®¡è®¡å™¨"""

    def __init__(self):
        self.file_filter = SmartFileFilter()
        self.large_file_handler = LargeFileHandler()
        self.security_analyzer = SecurityAnalyzer()
        self.results = []
        self.stats = {
            'total_files_scanned': 0,
            'files_filtered': 0,
            'files_analyzed': 0,
            'large_files_chunked': 0,
            'total_issues_found': 0,
            'analysis_time': 0
        }

    def scan_project(self, project_path: str) -> List[FileInfo]:
        """æ‰«æé¡¹ç›®æ–‡ä»¶"""
        logger.info(f"å¼€å§‹æ‰«æé¡¹ç›®: {project_path}")

        if not os.path.exists(project_path):
            raise ValueError(f"é¡¹ç›®è·¯å¾„ä¸å­˜åœ¨: {project_path}")

        files_to_analyze = []
        filter_stats = {}

        start_time = time.time()

        for root, dirs, files in os.walk(project_path):
            for file in files:
                file_path = os.path.join(root, file)
                self.stats['total_files_scanned'] += 1

                should_filter, reason = self.file_filter.should_filter_file(file_path, project_path)

                if should_filter:
                    self.stats['files_filtered'] += 1
                    filter_stats[reason] = filter_stats.get(reason, 0) + 1
                else:
                    try:
                        size = os.path.getsize(file_path)
                        file_info = FileInfo(
                            path=os.path.relpath(file_path, project_path),
                            full_path=file_path,
                            size=size,
                            size_mb=size / 1024 / 1024
                        )
                        files_to_analyze.append(file_info)
                    except Exception as e:
                        logger.warning(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯: {file_path} - {e}")

        scan_time = time.time() - start_time

        logger.info(f"æ‰«æå®Œæˆ: {scan_time:.2f}ç§’")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files_scanned']}")
        logger.info(f"è¿‡æ»¤æ–‡ä»¶æ•°: {self.stats['files_filtered']}")
        logger.info(f"å¾…åˆ†ææ–‡ä»¶æ•°: {len(files_to_analyze)}")

        logger.info("è¿‡æ»¤ç»Ÿè®¡:")
        for reason, count in sorted(filter_stats.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {reason}: {count}ä¸ª")

        return files_to_analyze

    def analyze_file(self, file_info: FileInfo) -> AuditResult:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        start_time = time.time()

        try:
            with open(file_info.full_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—
            if self.large_file_handler.should_chunk_file(file_info.size):
                logger.info(f"å¤§æ–‡ä»¶åˆ†å—å¤„ç†: {file_info.path} ({file_info.size_mb:.1f}MB)")
                chunks = self.large_file_handler.chunk_file_content(content)
                self.stats['large_files_chunked'] += 1

                all_issues = []
                for i, chunk in enumerate(chunks):
                    chunk_issues = self.security_analyzer.analyze_content(chunk, file_info.path)
                    all_issues.extend(chunk_issues)
                    logger.debug(f"  å— {i+1}/{len(chunks)}: {len(chunk_issues)} ä¸ªé—®é¢˜")
            else:
                all_issues = self.security_analyzer.analyze_content(content, file_info.path)

            analysis_time = time.time() - start_time
            lines_count = len(content.split('\n'))

            # ç”Ÿæˆæ‘˜è¦
            issue_counts = {}
            for issue in all_issues:
                issue_counts[issue.type] = issue_counts.get(issue.type, 0) + 1

            summary_parts = []
            for issue_type, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
                summary_parts.append(f"{issue_type}: {count}")

            summary = f"å‘ç° {len(all_issues)} ä¸ªå®‰å…¨é—®é¢˜ã€‚" + (
                " ä¸»è¦é—®é¢˜: " + ", ".join(summary_parts[:3]) if summary_parts else ""
            )

            result = AuditResult(
                file_path=file_info.path,
                language=file_info.language,
                issues=all_issues,
                summary=summary,
                lines_analyzed=lines_count,
                analysis_time=analysis_time
            )

            self.stats['total_issues_found'] += len(all_issues)
            self.stats['analysis_time'] += analysis_time

            return result

        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶å¤±è´¥: {file_info.path} - {e}")
            return AuditResult(
                file_path=file_info.path,
                language=file_info.language,
                issues=[],
                summary=f"åˆ†æå¤±è´¥: {str(e)}",
                lines_analyzed=0,
                analysis_time=time.time() - start_time
            )

    def run_audit(self, project_path: str, max_files: int = 0) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®¡è®¡"""
        logger.info("ğŸš€ å¼€å§‹qdbcrm-v3.0.2å®Œæ•´ä»£ç å®¡è®¡")
        logger.info("=" * 60)

        audit_start_time = time.time()

        # 1. æ‰«æé¡¹ç›®
        files_to_analyze = self.scan_project(project_path)

        # 2. é™åˆ¶æ–‡ä»¶æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if max_files > 0:
            files_to_analyze = files_to_analyze[:max_files]
            logger.info(f"é™åˆ¶åˆ†ææ–‡ä»¶æ•°: {max_files}")

        # 3. åˆ†ææ–‡ä»¶
        logger.info(f"\nğŸ” å¼€å§‹å®‰å…¨åˆ†æ ({len(files_to_analyze)} ä¸ªæ–‡ä»¶)")
        logger.info("-" * 40)

        for i, file_info in enumerate(files_to_analyze, 1):
            logger.info(f"[{i}/{len(files_to_analyze)}] åˆ†æ: {file_info.path} ({file_info.size_mb:.2f}MB)")

            result = self.analyze_file(file_info)
            self.results.append(result)
            self.stats['files_analyzed'] += 1

            if result.issues:
                logger.info(f"  âœ… å®Œæˆ - å‘ç° {len(result.issues)} ä¸ªé—®é¢˜")
            else:
                logger.info(f"  âœ… å®Œæˆ - æœªå‘ç°é—®é¢˜")

        total_audit_time = time.time() - audit_start_time

        # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report = self.generate_report(project_path, total_audit_time)

        logger.info("\nğŸ¯ å®¡è®¡å®Œæˆ!")
        logger.info("=" * 60)
        logger.info(f"æ€»è€—æ—¶: {total_audit_time:.2f}ç§’")
        logger.info(f"åˆ†ææ–‡ä»¶: {self.stats['files_analyzed']}")
        logger.info(f"å‘ç°é—®é¢˜: {self.stats['total_issues_found']}")
        logger.info(f"å¤§æ–‡ä»¶åˆ†å—: {self.stats['large_files_chunked']}")

        return report

    def generate_report(self, project_path: str, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        issue_stats = {}
        severity_stats = {'Critical': 0, 'High': 0, 'Medium': 0, 'Low': 0}

        for result in self.results:
            for issue in result.issues:
                issue_stats[issue.type] = issue_stats.get(issue.type, 0) + 1
                severity_stats[issue.severity] = severity_stats.get(issue.severity, 0) + 1

        # æ‰¾å‡ºé«˜é£é™©æ–‡ä»¶
        high_risk_files = []
        for result in self.results:
            critical_issues = [i for i in result.issues if i.severity == 'Critical']
            high_issues = [i for i in result.issues if i.severity == 'High']

            if critical_issues or len(high_issues) > 5:
                high_risk_files.append({
                    'file': result.file_path,
                    'critical_issues': len(critical_issues),
                    'high_issues': len(high_issues),
                    'total_issues': len(result.issues)
                })

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'project_info': {
                'project_path': project_path,
                'audit_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_audit_time': total_time,
                'auditor_version': 'qdbcrm-auditor-v1.0'
            },
            'scan_statistics': {
                'total_files_scanned': self.stats['total_files_scanned'],
                'files_filtered': self.stats['files_filtered'],
                'files_analyzed': self.stats['files_analyzed'],
                'large_files_chunked': self.stats['large_files_chunked'],
                'filter_efficiency': (self.stats['files_filtered'] / self.stats['total_files_scanned']) * 100
            },
            'security_summary': {
                'total_issues': self.stats['total_issues_found'],
                'severity_distribution': severity_stats,
                'issue_categories': issue_stats,
                'high_risk_files_count': len(high_risk_files)
            },
            'high_risk_files': high_risk_files[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
            'detailed_results': [
                {
                    'file_path': result.file_path,
                    'language': result.language,
                    'summary': result.summary,
                    'lines_analyzed': result.lines_analyzed,
                    'analysis_time': result.analysis_time,
                    'issues_count': len(result.issues),
                    'issues': [
                        {
                            'type': issue.type,
                            'severity': issue.severity,
                            'description': issue.description,
                            'line_number': issue.line_number,
                            'code_snippet': issue.code_snippet
                        } for issue in result.issues[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    ]
                } for result in self.results
            ]
        }

        return report

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='qdbcrm-v3.0.2 å®Œæ•´ä»£ç å®¡è®¡')
    parser.add_argument('project_path', help='é¡¹ç›®è·¯å¾„')
    parser.add_argument('--max-files', type=int, default=0, help='æœ€å¤§åˆ†ææ–‡ä»¶æ•° (0=æ— é™åˆ¶)')
    parser.add_argument('--output-file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--show-filter-stats', action='store_true', help='æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡')

    args = parser.parse_args()

    try:
        auditor = QdbcrmAuditor()
        report = auditor.run_audit(args.project_path, args.max_files)

        # ä¿å­˜æŠ¥å‘Š
        if args.output_file:
            with open(args.output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {args.output_file}")

        # æ˜¾ç¤ºæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¯ å®¡è®¡æ‘˜è¦")
        print("="*60)
        print(f"é¡¹ç›®è·¯å¾„: {report['project_info']['project_path']}")
        print(f"å®¡è®¡æ—¶é—´: {report['project_info']['audit_time']}")
        print(f"æ€»è€—æ—¶: {report['project_info']['total_audit_time']:.2f}ç§’")
        print(f"æ‰«ææ–‡ä»¶: {report['scan_statistics']['total_files_scanned']}")
        print(f"è¿‡æ»¤æ–‡ä»¶: {report['scan_statistics']['files_filtered']}")
        print(f"åˆ†ææ–‡ä»¶: {report['scan_statistics']['files_analyzed']}")
        print(f"è¿‡æ»¤æ•ˆç‡: {report['scan_statistics']['filter_efficiency']:.1f}%")
        print(f"å‘ç°é—®é¢˜: {report['security_summary']['total_issues']}")
        print(f"é«˜é£é™©æ–‡ä»¶: {report['security_summary']['high_risk_files_count']}")

        print(f"\nğŸ” é—®é¢˜åˆ†å¸ƒ:")
        for severity, count in report['security_summary']['severity_distribution'].items():
            if count > 0:
                print(f"  {severity}: {count}")

        print(f"\nğŸ“Š é—®é¢˜ç±»å‹ (å‰5):")
        sorted_issues = sorted(report['security_summary']['issue_categories'].items(),
                             key=lambda x: x[1], reverse=True)
        for issue_type, count in sorted_issues[:5]:
            print(f"  {issue_type}: {count}")

        if report['high_risk_files']:
            print(f"\nâš ï¸  é«˜é£é™©æ–‡ä»¶ (å‰5):")
            for file_info in report['high_risk_files'][:5]:
                print(f"  {file_info['file']}: {file_info['total_issues']} é—®é¢˜")

        return 0

    except Exception as e:
        logger.error(f"å®¡è®¡å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())