#!/usr/bin/env python3
"""
æµ‹è¯•APIæ”¹è¿›æ•ˆæœçš„è„šæœ¬
"""
import asyncio
import time
import logging
from ai_code_audit.llm.manager import LLMManager
from ai_code_audit.llm.models import LLMRequest, LLMMessage, MessageRole, LLMModelType

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_api_improvements():
    """æµ‹è¯•APIæ”¹è¿›æ•ˆæœ"""
    print("ğŸ§ª æµ‹è¯•APIæ”¹è¿›æ•ˆæœ")
    print("=" * 50)
    
    # åˆå§‹åŒ–LLMç®¡ç†å™¨
    llm_manager = LLMManager()
    
    # åˆ›å»ºæµ‹è¯•è¯·æ±‚
    test_requests = []
    for i in range(20):  # åˆ›å»º20ä¸ªè¯·æ±‚æ¥æµ‹è¯•TPMé™åˆ¶ä¼˜åŒ–
        request = LLMRequest(
            messages=[
                LLMMessage(MessageRole.SYSTEM, "ä½ æ˜¯ä¸€ä¸ªä»£ç å®‰å…¨å®¡è®¡ä¸“å®¶"),
                LLMMessage(MessageRole.USER, f"è¯·åˆ†æè¿™æ®µä»£ç çš„å®‰å…¨æ€§ #{i+1}: console.log('test');")
            ],
            model=LLMModelType.KIMI_K2,
            temperature=0.1,
            max_tokens=100
        )
        test_requests.append(request)
    
    print(f"ğŸ“Š åˆå§‹ç»Ÿè®¡:")
    stats = llm_manager.get_comprehensive_stats()
    print(f"  å¹¶å‘: {stats['concurrency']['current_concurrency']}")
    print(f"  TPMä½¿ç”¨ç‡: {stats['rate_limits']['tpm_usage_percent']:.1f}%")
    print(f"  RPMä½¿ç”¨ç‡: {stats['rate_limits']['rpm_usage_percent']:.1f}%")
    
    # å¹¶å‘æ‰§è¡Œè¯·æ±‚
    start_time = time.time()
    
    async def execute_request(req, index):
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œè¯·æ±‚ #{index+1}")
            response = await llm_manager.chat_completion(req)
            logger.info(f"è¯·æ±‚ #{index+1} å®Œæˆï¼Œå“åº”é•¿åº¦: {len(response.content)}")
            return True, None
        except Exception as e:
            logger.error(f"è¯·æ±‚ #{index+1} å¤±è´¥: {e}")
            return False, str(e)
    
    # æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
    tasks = [execute_request(req, i) for i, req in enumerate(test_requests)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for r in results if isinstance(r, tuple) and r[0])
    failed = len(results) - successful
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  æ€»è¯·æ±‚æ•°: {len(test_requests)}")
    print(f"  æˆåŠŸè¯·æ±‚: {successful}")
    print(f"  å¤±è´¥è¯·æ±‚: {failed}")
    print(f"  æˆåŠŸç‡: {successful/len(test_requests)*100:.1f}%")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  å¹³å‡è€—æ—¶: {total_time/len(test_requests):.2f}ç§’/è¯·æ±‚")
    
    # è¯¦ç»†ç»Ÿè®¡
    final_stats = llm_manager.get_comprehensive_stats()
    print(f"\nğŸ”§ æœ€ç»ˆç»Ÿè®¡:")
    print(f"  å¹¶å‘æ§åˆ¶:")
    for key, value in final_stats['concurrency'].items():
        print(f"    {key}: {value}")

    print(f"  é™æµç»Ÿè®¡:")
    for key, value in final_stats['rate_limits'].items():
        if 'percent' in key:
            print(f"    {key}: {value:.1f}%")
        else:
            print(f"    {key}: {value}")

    print(f"  æä¾›å•†ç»Ÿè®¡:")
    for key, value in final_stats['providers'].items():
        print(f"    {key}: {value}")
    
    # å¤±è´¥è¯¦æƒ…
    if failed > 0:
        print(f"\nâŒ å¤±è´¥è¯¦æƒ…:")
        for i, result in enumerate(results):
            if isinstance(result, tuple) and not result[0]:
                print(f"  è¯·æ±‚ #{i+1}: {result[1]}")
            elif isinstance(result, Exception):
                print(f"  è¯·æ±‚ #{i+1}: {result}")
    
    await llm_manager.close()


async def test_error_recovery():
    """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
    print("\nğŸ”„ æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶")
    print("=" * 50)
    
    llm_manager = LLMManager()
    
    # åˆ›å»ºä¸€ä¸ªå¯èƒ½å¯¼è‡´é”™è¯¯çš„è¯·æ±‚ï¼ˆè¶…é•¿å†…å®¹ï¼‰
    long_content = "åˆ†æè¿™æ®µä»£ç : " + "x" * 10000  # è¶…é•¿å†…å®¹å¯èƒ½å¯¼è‡´é”™è¯¯
    
    request = LLMRequest(
        messages=[
            LLMMessage(MessageRole.SYSTEM, "ä½ æ˜¯ä¸€ä¸ªä»£ç å®‰å…¨å®¡è®¡ä¸“å®¶"),
            LLMMessage(MessageRole.USER, long_content)
        ],
        model=LLMModelType.KIMI_K2,
        temperature=0.1,
        max_tokens=50
    )
    
    try:
        print("ğŸ“¤ å‘é€å¯èƒ½å¤±è´¥çš„è¯·æ±‚...")
        response = await llm_manager.chat_completion(request)
        print(f"âœ… è¯·æ±‚æˆåŠŸ: {response.content[:100]}...")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        print("ğŸ”§ æ£€æŸ¥é‡è¯•æœºåˆ¶æ˜¯å¦æ­£å¸¸å·¥ä½œ...")
    
    await llm_manager.close()


async def test_circuit_breaker():
    """æµ‹è¯•ç†”æ–­å™¨æœºåˆ¶"""
    print("\nâš¡ æµ‹è¯•ç†”æ–­å™¨æœºåˆ¶")
    print("=" * 50)
    
    llm_manager = LLMManager()
    
    # åˆ›å»ºå¤šä¸ªå¯èƒ½å¤±è´¥çš„è¯·æ±‚æ¥è§¦å‘ç†”æ–­å™¨
    invalid_requests = []
    for i in range(8):  # åˆ›å»º8ä¸ªè¯·æ±‚ï¼Œè¶…è¿‡ç†”æ–­é˜ˆå€¼
        request = LLMRequest(
            messages=[
                LLMMessage(MessageRole.SYSTEM, ""),  # ç©ºç³»ç»Ÿæ¶ˆæ¯å¯èƒ½å¯¼è‡´é”™è¯¯
                LLMMessage(MessageRole.USER, "")     # ç©ºç”¨æˆ·æ¶ˆæ¯å¯èƒ½å¯¼è‡´é”™è¯¯
            ],
            model=LLMModelType.KIMI_K2,
            temperature=0.1,
            max_tokens=10
        )
        invalid_requests.append(request)
    
    print("ğŸ“¤ å‘é€å¯èƒ½è§¦å‘ç†”æ–­å™¨çš„è¯·æ±‚...")
    
    for i, request in enumerate(invalid_requests):
        try:
            response = await llm_manager.chat_completion(request)
            print(f"âœ… è¯·æ±‚ #{i+1} æˆåŠŸ")
        except Exception as e:
            print(f"âŒ è¯·æ±‚ #{i+1} å¤±è´¥: {e}")
        
        # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
        stats = llm_manager.get_concurrency_stats()
        print(f"ğŸ”§ ç†”æ–­å™¨çŠ¶æ€: {stats.get('circuit_breaker_state', 'unknown')}")
        
        if stats.get('circuit_breaker_state') == 'open':
            print("âš¡ ç†”æ–­å™¨å·²æ‰“å¼€ï¼Œåœæ­¢å‘é€è¯·æ±‚")
            break
        
        await asyncio.sleep(1)  # é—´éš”1ç§’
    
    await llm_manager.close()


if __name__ == "__main__":
    async def main():
        await test_api_improvements()
        await test_error_recovery()
        await test_circuit_breaker()
    
    asyncio.run(main())
