"""
Multi-Round Analysis Engine
å¤šè½®åˆ†æå¼•æ“ - æ¸è¿›å¼æ·±åº¦å®‰å…¨åˆ†æ
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass
from enum import Enum

from ..core.models import FileInfo, AnalysisResult
from ..llm.client import LLMClient
from ..llm.prompts import PromptManager
from ..detection.advanced_patterns import AdvancedPatternDetector

logger = logging.getLogger(__name__)


class AnalysisRound(Enum):
    """åˆ†æè½®æ¬¡"""
    QUICK_SCAN = "quick_scan"           # å¿«é€Ÿæ‰«æ
    DEEP_ANALYSIS = "deep_analysis"     # æ·±åº¦åˆ†æ
    EXPERT_REVIEW = "expert_review"     # ä¸“å®¶çº§å®¡æŸ¥
    CROSS_FILE = "cross_file"           # è·¨æ–‡ä»¶åˆ†æ
    BUSINESS_LOGIC = "business_logic"   # ä¸šåŠ¡é€»è¾‘åˆ†æ


@dataclass
class RoundResult:
    """å•è½®åˆ†æç»“æœ"""
    round_type: AnalysisRound
    findings: List[Dict]
    confidence_score: float
    analysis_time: float
    tokens_used: int
    suspicious_areas: List[Dict]


class MultiRoundAnalyzer:
    """å¤šè½®åˆ†æå¼•æ“"""
    
    def __init__(self, llm_client: LLMClient, prompt_manager: PromptManager):
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.pattern_detector = AdvancedPatternDetector()
        self.analysis_history = []
        
        # åˆ†æé˜ˆå€¼é…ç½®
        self.thresholds = {
            "suspicious_score": 0.6,      # å¯ç–‘åº¦é˜ˆå€¼
            "critical_file_score": 0.8,   # å…³é”®æ–‡ä»¶é˜ˆå€¼
            "deep_analysis_trigger": 0.7, # æ·±åº¦åˆ†æè§¦å‘é˜ˆå€¼
            "expert_review_trigger": 0.9  # ä¸“å®¶å®¡æŸ¥è§¦å‘é˜ˆå€¼
        }
    
    async def analyze_with_multiple_rounds(
        self, 
        files: List[FileInfo], 
        max_rounds: int = 4
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šè½®åˆ†æ"""
        logger.info(f"å¼€å§‹å¤šè½®åˆ†æï¼Œæ–‡ä»¶æ•°é‡: {len(files)}, æœ€å¤§è½®æ¬¡: {max_rounds}")
        
        results = {
            "rounds": [],
            "consolidated_findings": [],
            "analysis_metadata": {
                "total_files": len(files),
                "rounds_executed": 0,
                "total_tokens": 0,
                "total_time": 0
            }
        }
        
        # ç¬¬ä¸€è½®ï¼šå¿«é€Ÿæ¨¡å¼æ‰«æ
        round1_result = await self._round1_quick_scan(files)
        results["rounds"].append(round1_result)
        results["analysis_metadata"]["rounds_executed"] += 1
        
        # æ ¹æ®ç¬¬ä¸€è½®ç»“æœå†³å®šæ˜¯å¦è¿›è¡Œåç»­åˆ†æ
        high_risk_files = self._identify_high_risk_files(files, round1_result)
        
        if high_risk_files and max_rounds > 1:
            # ç¬¬äºŒè½®ï¼šæ·±åº¦AIåˆ†æ
            round2_result = await self._round2_deep_analysis(high_risk_files, round1_result)
            results["rounds"].append(round2_result)
            results["analysis_metadata"]["rounds_executed"] += 1
            
            if self._should_continue_analysis(round2_result) and max_rounds > 2:
                # ç¬¬ä¸‰è½®ï¼šè·¨æ–‡ä»¶å…³è”åˆ†æ
                round3_result = await self._round3_cross_file_analysis(files, round2_result)
                results["rounds"].append(round3_result)
                results["analysis_metadata"]["rounds_executed"] += 1
                
                if self._needs_expert_review(round3_result) and max_rounds > 3:
                    # ç¬¬å››è½®ï¼šä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘åˆ†æ
                    round4_result = await self._round4_expert_business_logic(files, round3_result)
                    results["rounds"].append(round4_result)
                    results["analysis_metadata"]["rounds_executed"] += 1
        
        # æ•´åˆæ‰€æœ‰è½®æ¬¡çš„ç»“æœ
        results["consolidated_findings"] = self._consolidate_findings(results["rounds"])
        results["analysis_metadata"]["total_tokens"] = sum(r.tokens_used for r in results["rounds"])
        results["analysis_metadata"]["total_time"] = sum(r.analysis_time for r in results["rounds"])
        
        logger.info(f"å¤šè½®åˆ†æå®Œæˆï¼Œæ‰§è¡Œè½®æ¬¡: {results['analysis_metadata']['rounds_executed']}")
        return results
    
    async def _round1_quick_scan(self, files: List[FileInfo]) -> RoundResult:
        """ç¬¬ä¸€è½®ï¼šå¿«é€Ÿæ¨¡å¼æ‰«æ"""
        logger.info("æ‰§è¡Œç¬¬ä¸€è½®ï¼šå¿«é€Ÿæ¨¡å¼æ‰«æ")
        import time
        start_time = time.time()
        
        findings = []
        suspicious_areas = []
        
        for file_info in files:
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_info.absolute_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ä½¿ç”¨é«˜çº§æ¨¡å¼æ£€æµ‹å™¨
                pattern_findings = self.pattern_detector.detect_advanced_vulnerabilities(
                    content, file_info.path
                )
                findings.extend(pattern_findings)
                
                # ä¸šåŠ¡é€»è¾‘ä¸Šä¸‹æ–‡åˆ†æ
                business_findings = self.pattern_detector.analyze_business_logic_context(
                    content, file_info.path
                )
                findings.extend(business_findings)
                
                # è®¡ç®—æ–‡ä»¶å¯ç–‘åº¦
                suspicious_score = self._calculate_file_suspicion_score(pattern_findings + business_findings)
                if suspicious_score > self.thresholds["suspicious_score"]:
                    suspicious_areas.append({
                        "file": file_info.path,
                        "score": suspicious_score,
                        "reasons": [f["type"] for f in pattern_findings + business_findings]
                    })
                
            except Exception as e:
                logger.error(f"ç¬¬ä¸€è½®åˆ†ææ–‡ä»¶ {file_info.path} æ—¶å‡ºé”™: {e}")
        
        analysis_time = time.time() - start_time
        
        return RoundResult(
            round_type=AnalysisRound.QUICK_SCAN,
            findings=findings,
            confidence_score=0.8,  # æ¨¡å¼åŒ¹é…çš„ç½®ä¿¡åº¦è¾ƒé«˜
            analysis_time=analysis_time,
            tokens_used=0,  # æ¨¡å¼åŒ¹é…ä¸ä½¿ç”¨tokens
            suspicious_areas=suspicious_areas
        )
    
    async def _round2_deep_analysis(
        self, 
        high_risk_files: List[FileInfo], 
        round1_result: RoundResult
    ) -> RoundResult:
        """ç¬¬äºŒè½®ï¼šæ·±åº¦AIåˆ†æ"""
        logger.info(f"æ‰§è¡Œç¬¬äºŒè½®ï¼šæ·±åº¦AIåˆ†æï¼Œæ–‡ä»¶æ•°é‡: {len(high_risk_files)}")
        import time
        start_time = time.time()
        
        findings = []
        total_tokens = 0
        
        # è·å–å¢å¼ºçš„å®‰å…¨å®¡è®¡æ¨¡æ¿
        template = self.prompt_manager.get_template("security_audit_enhanced")
        if not template:
            template = self.prompt_manager.get_template("security_audit")
        
        for file_info in high_risk_files:
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_info.absolute_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                round1_findings = [f for f in round1_result.findings if f.get("location", {}).get("file") == file_info.path]
                context = self._build_enhanced_context(file_info, round1_findings)
                
                # ç”Ÿæˆæ·±åº¦åˆ†ææç¤º
                prompt = template.format_prompt(
                    language=file_info.language or 'python',
                    file_path=file_info.path,
                    project_type='web_application',
                    dependencies='flask, sqlite3, subprocess',
                    code_content=content,
                    additional_context=context
                )
                
                # æ‰§è¡ŒAIåˆ†æ
                response = await self.llm_client.generate_response(
                    prompt.system_prompt,
                    prompt.user_prompt,
                    temperature=0.05,  # æ›´ä½çš„æ¸©åº¦ä»¥æé«˜å‡†ç¡®æ€§
                    max_tokens=4096
                )
                
                # è§£æAIå“åº”
                ai_findings = self._parse_ai_response(response, file_info.path)
                findings.extend(ai_findings)
                total_tokens += len(response.split())  # ç®€å•çš„tokenä¼°ç®—
                
            except Exception as e:
                logger.error(f"ç¬¬äºŒè½®åˆ†ææ–‡ä»¶ {file_info.path} æ—¶å‡ºé”™: {e}")
        
        analysis_time = time.time() - start_time
        
        return RoundResult(
            round_type=AnalysisRound.DEEP_ANALYSIS,
            findings=findings,
            confidence_score=0.9,  # AIåˆ†æçš„ç½®ä¿¡åº¦å¾ˆé«˜
            analysis_time=analysis_time,
            tokens_used=total_tokens,
            suspicious_areas=[]
        )
    
    async def _round3_cross_file_analysis(
        self, 
        files: List[FileInfo], 
        round2_result: RoundResult
    ) -> RoundResult:
        """ç¬¬ä¸‰è½®ï¼šè·¨æ–‡ä»¶å…³è”åˆ†æ"""
        logger.info("æ‰§è¡Œç¬¬ä¸‰è½®ï¼šè·¨æ–‡ä»¶å…³è”åˆ†æ")
        import time
        start_time = time.time()
        
        findings = []
        
        # æ„å»ºæ–‡ä»¶ä¾èµ–å›¾
        dependency_graph = self._build_dependency_graph(files)
        
        # åˆ†æè·¨æ–‡ä»¶æ¼æ´é“¾
        vulnerability_chains = self._analyze_vulnerability_chains(
            dependency_graph, 
            round2_result.findings
        )
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        for chain in vulnerability_chains:
            findings.append({
                "type": "cross_file_vulnerability_chain",
                "severity": chain["severity"],
                "description": chain["description"],
                "files_involved": chain["files"],
                "attack_path": chain["path"],
                "business_impact": chain["impact"],
                "remediation": "å®æ–½è·¨æ¨¡å—çš„å®‰å…¨éªŒè¯å’Œæ•°æ®æµæ§åˆ¶"
            })
        
        analysis_time = time.time() - start_time
        
        return RoundResult(
            round_type=AnalysisRound.CROSS_FILE,
            findings=findings,
            confidence_score=0.85,
            analysis_time=analysis_time,
            tokens_used=0,
            suspicious_areas=[]
        )
    
    async def _round4_expert_business_logic(
        self, 
        files: List[FileInfo], 
        round3_result: RoundResult
    ) -> RoundResult:
        """ç¬¬å››è½®ï¼šä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘åˆ†æ"""
        logger.info("æ‰§è¡Œç¬¬å››è½®ï¼šä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘åˆ†æ")
        import time
        start_time = time.time()
        
        findings = []
        total_tokens = 0
        
        # ä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘åˆ†ææç¤º
        expert_prompt = self._create_expert_business_logic_prompt()
        
        # åˆ†æå…³é”®ä¸šåŠ¡é€»è¾‘æ–‡ä»¶
        business_critical_files = [f for f in files if self._is_business_critical(f)]
        
        for file_info in business_critical_files:
            try:
                with open(file_info.absolute_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ„å»ºä¸“å®¶çº§åˆ†æä¸Šä¸‹æ–‡
                expert_context = self._build_expert_context(file_info, round3_result.findings)
                
                # æ‰§è¡Œä¸“å®¶çº§AIåˆ†æ
                response = await self.llm_client.generate_response(
                    expert_prompt,
                    f"""
                    æ–‡ä»¶: {file_info.path}
                    ä¸Šä¸‹æ–‡: {expert_context}
                    
                    ä»£ç :
                    ```python
                    {content}
                    ```
                    
                    è¯·è¿›è¡Œä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘å®‰å…¨åˆ†æã€‚
                    """,
                    temperature=0.02,  # æä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
                    max_tokens=3072
                )
                
                expert_findings = self._parse_expert_response(response, file_info.path)
                findings.extend(expert_findings)
                total_tokens += len(response.split())
                
            except Exception as e:
                logger.error(f"ç¬¬å››è½®åˆ†ææ–‡ä»¶ {file_info.path} æ—¶å‡ºé”™: {e}")
        
        analysis_time = time.time() - start_time
        
        return RoundResult(
            round_type=AnalysisRound.BUSINESS_LOGIC,
            findings=findings,
            confidence_score=0.95,  # ä¸“å®¶çº§åˆ†æç½®ä¿¡åº¦æœ€é«˜
            analysis_time=analysis_time,
            tokens_used=total_tokens,
            suspicious_areas=[]
        )
    
    def _identify_high_risk_files(self, files: List[FileInfo], round1_result: RoundResult) -> List[FileInfo]:
        """è¯†åˆ«é«˜é£é™©æ–‡ä»¶"""
        high_risk_files = []
        
        # åŸºäºç¬¬ä¸€è½®ç»“æœè¯†åˆ«é«˜é£é™©æ–‡ä»¶
        suspicious_file_paths = {area["file"] for area in round1_result.suspicious_areas}
        
        for file_info in files:
            if (file_info.path in suspicious_file_paths or 
                self._is_critical_file(file_info) or
                self._has_high_severity_findings(file_info.path, round1_result.findings)):
                high_risk_files.append(file_info)
        
        return high_risk_files
    
    def _calculate_file_suspicion_score(self, findings: List[Dict]) -> float:
        """è®¡ç®—æ–‡ä»¶å¯ç–‘åº¦åˆ†æ•°"""
        if not findings:
            return 0.0
        
        score = 0.0
        for finding in findings:
            severity = finding.get("severity", "low")
            if severity == "critical":
                score += 0.4
            elif severity == "high":
                score += 0.3
            elif severity == "medium":
                score += 0.2
            else:
                score += 0.1
        
        return min(score, 1.0)
    
    def _build_enhanced_context(self, file_info: FileInfo, round1_findings: List[Dict]) -> str:
        """æ„å»ºå¢å¼ºçš„åˆ†æä¸Šä¸‹æ–‡"""
        context_parts = [
            f"æ–‡ä»¶ {file_info.path} çš„æ·±åº¦å®‰å…¨åˆ†æ",
            f"ç¬¬ä¸€è½®æ‰«æå‘ç° {len(round1_findings)} ä¸ªæ½œåœ¨é—®é¢˜ï¼š"
        ]
        
        for finding in round1_findings[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            context_parts.append(f"- {finding.get('type', 'unknown')}: {finding.get('description', '')}")
        
        context_parts.append("è¯·è¿›è¡Œæ›´æ·±å…¥çš„åˆ†æï¼Œç‰¹åˆ«å…³æ³¨ä¸šåŠ¡é€»è¾‘æ¼æ´å’Œå¤æ‚æ”»å‡»åœºæ™¯ã€‚")
        
        return "\n".join(context_parts)
    
    def _should_continue_analysis(self, round_result: RoundResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­åˆ†æ"""
        # å¦‚æœå‘ç°äº†é«˜ä¸¥é‡æ€§æ¼æ´ï¼Œç»§ç»­åˆ†æ
        critical_findings = [f for f in round_result.findings if f.get("severity") == "critical"]
        return len(critical_findings) > 0 or round_result.confidence_score > self.thresholds["deep_analysis_trigger"]
    
    def _needs_expert_review(self, round_result: RoundResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸“å®¶çº§å®¡æŸ¥"""
        return round_result.confidence_score > self.thresholds["expert_review_trigger"]
    
    def _is_critical_file(self, file_info: FileInfo) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®æ–‡ä»¶"""
        critical_patterns = ['auth', 'login', 'admin', 'payment', 'security', 'database']
        return any(pattern in file_info.path.lower() for pattern in critical_patterns)
    
    def _is_business_critical(self, file_info: FileInfo) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸šåŠ¡å…³é”®æ–‡ä»¶"""
        business_patterns = ['auth', 'payment', 'order', 'user', 'admin', 'workflow']
        return any(pattern in file_info.path.lower() for pattern in business_patterns)
    
    def _has_high_severity_findings(self, file_path: str, findings: List[Dict]) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰é«˜ä¸¥é‡æ€§å‘ç°"""
        file_findings = [f for f in findings if f.get("location", {}).get("file") == file_path]
        return any(f.get("severity") in ["critical", "high"] for f in file_findings)
    
    def _build_dependency_graph(self, files: List[FileInfo]) -> Dict:
        """æ„å»ºæ–‡ä»¶ä¾èµ–å›¾"""
        # ç®€åŒ–çš„ä¾èµ–å›¾æ„å»º
        graph = {"nodes": [], "edges": []}
        
        for file_info in files:
            graph["nodes"].append(file_info.path)
            
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ä¾èµ–åˆ†æ
            # ä¾‹å¦‚åˆ†æimportè¯­å¥ã€å‡½æ•°è°ƒç”¨ç­‰
        
        return graph
    
    def _analyze_vulnerability_chains(self, dependency_graph: Dict, findings: List[Dict]) -> List[Dict]:
        """åˆ†ææ¼æ´é“¾"""
        chains = []
        
        # ç®€åŒ–çš„æ¼æ´é“¾åˆ†æ
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæœ‰æ›´å¤æ‚çš„é€»è¾‘
        
        return chains
    
    def _create_expert_business_logic_prompt(self) -> str:
        """åˆ›å»ºä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘åˆ†ææç¤º"""
        return """ä½ æ˜¯ä¸€ä½æ‹¥æœ‰20å¹´ç»éªŒçš„é«˜çº§å®‰å…¨æ¶æ„å¸ˆå’Œä¸šåŠ¡é€»è¾‘å®‰å…¨ä¸“å®¶ã€‚

ğŸ¯ ä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘å®‰å…¨åˆ†æä»»åŠ¡ï¼š

é‡ç‚¹å…³æ³¨ä»¥ä¸‹é«˜çº§å®‰å…¨é—®é¢˜ï¼š

1. **å·¥ä½œæµå®‰å…¨ç¼ºé™·**ï¼š
   - æ­¥éª¤è·³è¿‡å’Œç»•è¿‡
   - çŠ¶æ€æœºçš„éæ³•è½¬æ¢
   - å¹¶å‘è®¿é—®çš„ç«æ€æ¡ä»¶

2. **æƒé™å’Œè®¿é—®æ§åˆ¶**ï¼š
   - å‚ç›´æƒé™æå‡
   - æ°´å¹³æƒé™ç»•è¿‡
   - ä¸Šä¸‹æ–‡ç›¸å…³çš„è®¿é—®æ§åˆ¶ç¼ºé™·

3. **ä¸šåŠ¡é€»è¾‘æ“çºµ**ï¼š
   - ä»·æ ¼å’Œæ•°é‡æ“çºµ
   - æ—¶é—´å’Œé¡ºåºæ“çºµ
   - ä¸šåŠ¡è§„åˆ™ç»•è¿‡

4. **æ•°æ®å®Œæ•´æ€§**ï¼š
   - å…³é”®ä¸šåŠ¡æ•°æ®çš„ç¯¡æ”¹
   - å®¡è®¡æ—¥å¿—çš„ç»•è¿‡
   - æ•°æ®ä¸€è‡´æ€§é—®é¢˜

è¯·æä¾›ä¸“å®¶çº§çš„åˆ†æï¼ŒåŒ…æ‹¬å…·ä½“çš„æ”»å‡»åœºæ™¯å’Œä¸šåŠ¡å½±å“è¯„ä¼°ã€‚"""
    
    def _build_expert_context(self, file_info: FileInfo, findings: List[Dict]) -> str:
        """æ„å»ºä¸“å®¶çº§åˆ†æä¸Šä¸‹æ–‡"""
        return f"åŸºäºå‰æœŸåˆ†æï¼Œæ–‡ä»¶ {file_info.path} éœ€è¦ä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘å®¡æŸ¥ã€‚"
    
    def _parse_ai_response(self, response: str, file_path: str) -> List[Dict]:
        """è§£æAIå“åº”"""
        # ç®€åŒ–çš„å“åº”è§£æ
        # å®é™…å®ç°ä¸­éœ€è¦æ›´å¤æ‚çš„è§£æé€»è¾‘
        return [{
            "type": "ai_detected_vulnerability",
            "severity": "medium",
            "description": "AIæ£€æµ‹åˆ°çš„æ½œåœ¨å®‰å…¨é—®é¢˜",
            "file": file_path,
            "raw_response": response[:200] + "..." if len(response) > 200 else response
        }]
    
    def _parse_expert_response(self, response: str, file_path: str) -> List[Dict]:
        """è§£æä¸“å®¶çº§å“åº”"""
        return [{
            "type": "expert_business_logic_finding",
            "severity": "high",
            "description": "ä¸“å®¶çº§ä¸šåŠ¡é€»è¾‘åˆ†æå‘ç°",
            "file": file_path,
            "expert_analysis": response[:300] + "..." if len(response) > 300 else response
        }]
    
    def _consolidate_findings(self, rounds: List[RoundResult]) -> List[Dict]:
        """æ•´åˆæ‰€æœ‰è½®æ¬¡çš„å‘ç°"""
        all_findings = []
        
        for round_result in rounds:
            for finding in round_result.findings:
                finding["analysis_round"] = round_result.round_type.value
                finding["confidence"] = round_result.confidence_score
                all_findings.append(finding)
        
        # å»é‡å’Œä¼˜å…ˆçº§æ’åº
        return self._deduplicate_and_prioritize(all_findings)
    
    def _deduplicate_and_prioritize(self, findings: List[Dict]) -> List[Dict]:
        """å»é‡å’Œä¼˜å…ˆçº§æ’åº"""
        # ç®€åŒ–çš„å»é‡é€»è¾‘
        seen = set()
        unique_findings = []
        
        # æŒ‰ä¸¥é‡æ€§æ’åº
        severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        findings.sort(key=lambda x: severity_order.get(x.get("severity", "low"), 3))
        
        for finding in findings:
            key = f"{finding.get('type', '')}_{finding.get('file', '')}"
            if key not in seen:
                seen.add(key)
                unique_findings.append(finding)
        
        return unique_findings
