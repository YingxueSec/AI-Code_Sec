#!/usr/bin/env python3
"""
AIä»£ç å®¡è®¡æ€§èƒ½åˆ†æå·¥å…·
è¯¦ç»†åˆ†ææ¯ä¸ªæ­¥éª¤çš„æ—¶é—´æ¶ˆè€—
"""

import time
import asyncio
import json
from datetime import datetime
from ai_code_audit.core.config import ConfigManager
from ai_code_audit.llm.manager import LLMManager
from ai_code_audit.llm.models import LLMRequest, LLMMessage, MessageRole

class PerformanceAnalyzer:
    def __init__(self):
        self.timings = {}
        self.start_time = None
        
    def start_timer(self, step_name):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        print(f"â±ï¸  å¼€å§‹: {step_name} - {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")
        
    def end_timer(self, step_name):
        """ç»“æŸè®¡æ—¶"""
        if self.start_time:
            duration = time.time() - self.start_time
            self.timings[step_name] = duration
            print(f"âœ… å®Œæˆ: {step_name} - è€—æ—¶: {duration:.2f}ç§’")
            return duration
        return 0
    
    async def analyze_llm_performance(self):
        """åˆ†æLLM APIæ€§èƒ½"""
        print("\nğŸ” LLM APIæ€§èƒ½åˆ†æ:")
        
        # æµ‹è¯•é…ç½®åŠ è½½
        self.start_timer("é…ç½®åŠ è½½")
        config_manager = ConfigManager()
        config = config_manager.load_config()
        self.end_timer("é…ç½®åŠ è½½")
        
        # æµ‹è¯•LLMåˆå§‹åŒ–
        self.start_timer("LLMç®¡ç†å™¨åˆå§‹åŒ–")
        llm_manager = LLMManager(config)
        self.end_timer("LLMç®¡ç†å™¨åˆå§‹åŒ–")
        
        # æµ‹è¯•å°è¯·æ±‚
        self.start_timer("å°è¯·æ±‚æµ‹è¯• (50 tokens)")
        small_request = LLMRequest(
            messages=[
                LLMMessage(MessageRole.SYSTEM, "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"),
                LLMMessage(MessageRole.USER, "è¯·ç®€å•å›å¤ï¼šæµ‹è¯•æˆåŠŸ")
            ],
            model='kimi-k2',
            temperature=0.1,
            max_tokens=50
        )
        
        try:
            response = await llm_manager.chat_completion(small_request)
            small_duration = self.end_timer("å°è¯·æ±‚æµ‹è¯• (50 tokens)")
            print(f"   å“åº”å†…å®¹: {response.content[:50]}...")
            print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens if response.usage else 'N/A'}")
        except Exception as e:
            print(f"   âŒ å°è¯·æ±‚å¤±è´¥: {e}")
            small_duration = 0
        
        # æµ‹è¯•ä¸­ç­‰è¯·æ±‚
        self.start_timer("ä¸­ç­‰è¯·æ±‚æµ‹è¯• (500 tokens)")
        medium_request = LLMRequest(
            messages=[
                LLMMessage(MessageRole.SYSTEM, "ä½ æ˜¯ä¸€ä¸ªä»£ç å®‰å…¨ä¸“å®¶"),
                LLMMessage(MessageRole.USER, """è¯·åˆ†æä»¥ä¸‹Pythonä»£ç çš„å®‰å…¨é—®é¢˜ï¼š
```python
def login(username, password):
    query = f"SELECT * FROM users WHERE username='{username}' AND password='{password}'"
    cursor.execute(query)
    return cursor.fetchone()
```
è¯·ç®€è¦è¯´æ˜ä¸»è¦å®‰å…¨é£é™©ã€‚""")
            ],
            model='kimi-k2',
            temperature=0.1,
            max_tokens=500
        )
        
        try:
            response = await llm_manager.chat_completion(medium_request)
            medium_duration = self.end_timer("ä¸­ç­‰è¯·æ±‚æµ‹è¯• (500 tokens)")
            print(f"   å“åº”å†…å®¹: {response.content[:100]}...")
            print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens if response.usage else 'N/A'}")
        except Exception as e:
            print(f"   âŒ ä¸­ç­‰è¯·æ±‚å¤±è´¥: {e}")
            medium_duration = 0
        
        # æµ‹è¯•å¤§è¯·æ±‚ (æ¨¡æ‹Ÿå®é™…å®¡è®¡)
        self.start_timer("å¤§è¯·æ±‚æµ‹è¯• (4000 tokens)")
        large_code = """
def get_user_data(user_id):
    conn = sqlite3.connect('app.db')
    cursor = conn.cursor()
    sql = f"SELECT * FROM users WHERE id = {user_id}"
    cursor.execute(sql)
    return cursor.fetchall()

def validate_user(user_id):
    if str(user_id) in ['1', 'admin', '0']:
        return True
    return False

def process_upload(filename, content):
    cmd = f"file {filename} && echo 'processed'"
    result = subprocess.run(cmd, shell=True)
    return result
""" * 3  # é‡å¤3æ¬¡æ¨¡æ‹Ÿæ›´å¤§çš„ä»£ç 
        
        large_request = LLMRequest(
            messages=[
                LLMMessage(MessageRole.SYSTEM, "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ä»£ç å®‰å…¨å®¡è®¡ä¸“å®¶ï¼Œè¯·è¿›è¡Œæ·±åº¦å®‰å…¨åˆ†æ"),
                LLMMessage(MessageRole.USER, f"""è¯·å¯¹ä»¥ä¸‹Pythonä»£ç è¿›è¡Œå…¨é¢çš„å®‰å…¨å®¡è®¡ï¼š

```python
{large_code}
```

è¯·è¯¦ç»†åˆ†ææ‰€æœ‰å®‰å…¨æ¼æ´ï¼ŒåŒ…æ‹¬ï¼š
1. æ¼æ´ç±»å‹å’Œä¸¥é‡ç¨‹åº¦
2. å…·ä½“æ”»å‡»åœºæ™¯
3. ä¸šåŠ¡å½±å“
4. ä¿®å¤æ–¹æ¡ˆ

è¯·æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚""")
            ],
            model='kimi-k2',
            temperature=0.1,
            max_tokens=4000
        )
        
        try:
            response = await llm_manager.chat_completion(large_request)
            large_duration = self.end_timer("å¤§è¯·æ±‚æµ‹è¯• (4000 tokens)")
            print(f"   å“åº”å†…å®¹: {response.content[:100]}...")
            print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens if response.usage else 'N/A'}")
        except Exception as e:
            print(f"   âŒ å¤§è¯·æ±‚å¤±è´¥: {e}")
            large_duration = 0
        
        return {
            'small_request': small_duration,
            'medium_request': medium_duration, 
            'large_request': large_duration
        }
    
    def analyze_results(self, llm_timings):
        """åˆ†æç»“æœ"""
        print("\nğŸ“Š æ€§èƒ½åˆ†æç»“æœ:")
        print("=" * 50)
        
        print("ğŸ”§ ç³»ç»Ÿå¼€é”€:")
        for step, duration in self.timings.items():
            print(f"  {step}: {duration:.2f}ç§’")
        
        print("\nğŸŒ APIè¯·æ±‚æ€§èƒ½:")
        for request_type, duration in llm_timings.items():
            if duration > 0:
                print(f"  {request_type}: {duration:.2f}ç§’")
        
        # è®¡ç®—æ¯tokençš„å¹³å‡æ—¶é—´
        if llm_timings['large_request'] > 0:
            tokens_per_second = 4000 / llm_timings['large_request']
            print(f"\nâš¡ APIæ€§èƒ½æŒ‡æ ‡:")
            print(f"  å¤„ç†é€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")
            print(f"  æ¯1000 tokensè€—æ—¶: {1000/tokens_per_second:.1f}ç§’")
        
        # åˆ†æç“¶é¢ˆ
        total_system = sum(self.timings.values())
        total_api = sum(v for v in llm_timings.values() if v > 0)
        
        print(f"\nğŸ¯ ç“¶é¢ˆåˆ†æ:")
        print(f"  ç³»ç»Ÿå¼€é”€æ€»è®¡: {total_system:.2f}ç§’")
        print(f"  APIè¯·æ±‚æ€»è®¡: {total_api:.2f}ç§’")
        if total_api > 0:
            print(f"  APIå æ¯”: {(total_api/(total_system+total_api))*100:.1f}%")
        
        # å¹¶å‘æ•ˆæœåˆ†æ
        print(f"\nğŸš€ å¹¶å‘æ•ˆæœåˆ†æ:")
        print(f"  å•æ–‡ä»¶å¹³å‡æ—¶é—´: {llm_timings.get('large_request', 0):.1f}ç§’")
        print(f"  4æ–‡ä»¶ä¸²è¡Œæ—¶é—´: {llm_timings.get('large_request', 0) * 4:.1f}ç§’")
        print(f"  4æ–‡ä»¶å¹¶å‘æ—¶é—´: {llm_timings.get('large_request', 0):.1f}ç§’ (ç†è®ºå€¼)")
        print(f"  å®é™…å¹¶å‘æ—¶é—´: 150ç§’ (å—APIé™åˆ¶)")

async def main():
    analyzer = PerformanceAnalyzer()
    
    print("ğŸš€ å¼€å§‹AIä»£ç å®¡è®¡æ€§èƒ½åˆ†æ")
    print("=" * 50)
    
    # åˆ†æLLMæ€§èƒ½
    llm_timings = await analyzer.analyze_llm_performance()
    
    # åˆ†æç»“æœ
    analyzer.analyze_results(llm_timings)

if __name__ == "__main__":
    asyncio.run(main())
